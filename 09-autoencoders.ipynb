{"cells":[{"cell_type":"markdown","metadata":{"id":"D77xr49wBixL"},"source":["### INTRODUCTION TO *AUTOENCODERS*"]},{"cell_type":"markdown","source":["MAIN OBJECTIVES: \n","- construct a simple version of a (convolutional) autoencoder \n","- understand  and limitations of the model \n","- investigate the properties of representations of the model"],"metadata":{"id":"swTuEIi-W9xf"}},{"cell_type":"markdown","source":["The next few blocks set up the basics for training the models we are going to inspect later:\n","\n","- we make sure folders in the course repo are available;\n","- we load the MNIST dataset and check the data looks as expected;\n","- we define basic training and test functionalities we will use for learning the weights of our model\n","- we define standard training hyperparameters "],"metadata":{"id":"eNKf9wOwXnnG"}},{"cell_type":"code","source":["# comment if access to the course repo is already available\n","!git clone https://github.com/AlbertoCazzaniga/MHPC_2022_test"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vb785waOCNSC","executionInfo":{"status":"ok","timestamp":1653507933071,"user_tz":-120,"elapsed":1985,"user":{"displayName":"Alberto Cazzaniga","userId":"03821533878775354456"}},"outputId":"d5c19013-53a4-46bb-aebc-cdd646ad2f21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'MHPC_2022_test'...\n","remote: Enumerating objects: 103, done.\u001b[K\n","remote: Counting objects: 100% (24/24), done.\u001b[K\n","remote: Compressing objects: 100% (23/23), done.\u001b[K\n","remote: Total 103 (delta 7), reused 0 (delta 0), pack-reused 79\u001b[K\n","Receiving objects: 100% (103/103), 11.10 MiB | 18.05 MiB/s, done.\n","Resolving deltas: 100% (11/11), done.\n"]}]},{"cell_type":"code","source":["%cd MHPC_2022_test"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mtA6BmIuCRET","executionInfo":{"status":"ok","timestamp":1653507933072,"user_tz":-120,"elapsed":14,"user":{"displayName":"Alberto Cazzaniga","userId":"03821533878775354456"}},"outputId":"02a5a74e-c27f-4130-b52e-d7de74ce0400"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/MHPC_2022_test\n"]}]},{"cell_type":"code","source":["# basic imports, load MNIST\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torchsummary import summary\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from scripts import mnist #NOTE: might need to customize this\n","\n","minibatch_size_train = 128\n","minibatch_size_test = 128\n","\n","trainloader, testloader = mnist.get_data_auto(batch_size_train=minibatch_size_test, batch_size_test=minibatch_size_test)"],"metadata":{"id":"3eya2frFCZo1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fgZgAVOCBixS"},"source":["Handle devices; set parameters for training and for regulating model dimension. \n"]},{"cell_type":"code","source":["#make sure we handle correctly hardware resources depending on availability, and fix seed\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"],"metadata":{"id":"VvQjTiCgu1pf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CeukIbDeBixS"},"outputs":[],"source":["#training parameters\n","num_epochs = 15\n","learning_rate = 1e-3\n","\n","#model parameters\n","latent_dims = 10\n","capacity = 64"]},{"cell_type":"markdown","metadata":{"id":"WdvezaHOBixU"},"source":["Exercise 1. The autoencoder class.\n","\n","We will construct a convolutional encoder and decoder. In convolution layers, we increase the channels as we approach the bottleneck, but note that the total number of features still decreases, since the channels increase by a factor of 2 in each convolution, but the spatial size decreases by a factor of 4.\n","\n","- Given the symmetry in the definition of the autoencoder, deduce the correct dimensions for the layers of the encoder starting from the given decoder. \"Hint\": use the function summary as in previous notebook verify that the dimensions mathc."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hrk5tAMNBixV"},"outputs":[],"source":["class Encoder(nn.Module):\n","    '''\n","    Your code here\n","    '''\n","    def __init__(self):\n","        super(Encoder, self).__init__()\n","        c = capacity\n","        '''\n","        Complete the following lines with appropriate 2d convolutions\n","        \n","        self.conv1 =  \n","        self.conv2 = \n","        self.fc = \n","        '''\n","            \n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = x.view(x.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors\n","        x = self.fc(x)\n","        return x\n","\n","class Decoder(nn.Module):\n","    def __init__(self):\n","        super(Decoder, self).__init__()\n","        c = capacity\n","        self.fc = nn.Linear(in_features=latent_dims, out_features=c*2*7*7)\n","        self.conv2 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)\n","        self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=1, kernel_size=4, stride=2, padding=1)\n","            \n","    def forward(self, x):\n","        x = self.fc(x)\n","        x = x.view(x.size(0), capacity*2, 7, 7) # unflatten batch of feature vectors to a batch of multi-channel feature maps\n","        x = F.relu(self.conv2(x))\n","        x = torch.tanh(self.conv1(x)) # last layer before output is tanh, since the images are normalized and 0-centered\n","        return x\n","    \n","class Autoencoder(nn.Module):\n","    def __init__(self):\n","        super(Autoencoder, self).__init__()\n","        self.encoder = Encoder()\n","        self.decoder = Decoder()\n","    \n","    def forward(self, x):\n","        latent = self.encoder(x)\n","        x_recon = self.decoder(latent)\n","        return x_recon"]},{"cell_type":"code","source":["autoencoder = Autoencoder()\n","autoencoder = autoencoder.to(device)\n","\n","summary(autoencoder,(1,28,28,))\n","\n","num_params = sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)\n","print('Total number of parameters: %d' % num_params)"],"metadata":{"id":"-NjfnINOptrZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a9fQcnRsBixW"},"source":["The following block is a very simple version of training strategy. \n","\n","NOTE the choice of MSE loss function for computing the reconstruction error as described in class.\n","\n","REMEMBER to save (NOT only in Colab) the weights after 50 epochs!"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"uvPoMTAbBixX"},"outputs":[],"source":["optimizer = torch.optim.Adam(params=autoencoder.parameters(), lr=learning_rate, weight_decay=1e-5)\n","\n","# set to training mode\n","autoencoder.train()\n","\n","train_loss_avg = []\n","\n","print('Training ...')\n","for epoch in range(num_epochs):\n","    train_loss_avg.append(0)\n","    num_batches = 0\n","    \n","    for image_batch, _ in trainloader:\n","        \n","        image_batch = image_batch.to(device)\n","        \n","        # autoencoder reconstruction\n","        image_batch_recon = autoencoder(image_batch)\n","        \n","        # reconstruction error\n","        loss = F.mse_loss(image_batch_recon, image_batch)\n","        \n","        # backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        \n","        # one step of the optmizer (using the gradients from backpropagation)\n","        optimizer.step()\n","        \n","        train_loss_avg[-1] += loss.item()\n","        num_batches += 1\n","        \n","    train_loss_avg[-1] /= num_batches\n","    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))\n","\n","torch.save(autoencoder.state_dict(),\"./models/auto_cn2.pt\")"]},{"cell_type":"markdown","metadata":{"id":"Z6WO5McABixY"},"source":["We monitor the learning procedure by plotting the training loss."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fr-BHfxgBixY"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","#plt.ion()\n","\n","fig = plt.figure()\n","plt.plot(train_loss_avg)\n","plt.xlabel('Epochs')\n","plt.ylabel('Reconstruction error')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"pL1WdoQmBixZ"},"source":["And verifying that the test loss of the final model is compatible with the trainung one."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H0yrkIh1BixZ","outputId":"d2029e6c-8e32-4dbf-a74c-7a0e345f4166","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653508144272,"user_tz":-120,"elapsed":10508,"user":{"displayName":"Alberto Cazzaniga","userId":"03821533878775354456"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["average reconstruction error: 0.388615\n"]}],"source":["# set to evaluation mode\n","autoencoder.eval()\n","\n","test_loss_avg, num_batches = 0, 0\n","for image_batch, _ in testloader:\n","    \n","    with torch.no_grad():\n","\n","        image_batch = image_batch.to(device)\n","\n","        # autoencoder reconstruction\n","        image_batch_recon = autoencoder(image_batch)\n","\n","        # reconstruction error\n","        loss = F.mse_loss(image_batch_recon, image_batch)\n","\n","        test_loss_avg += loss.item()\n","        num_batches += 1\n","    \n","test_loss_avg /= num_batches\n","print('average reconstruction error: %f' % (test_loss_avg))"]},{"cell_type":"markdown","metadata":{"id":"XIOG_ubOBixa"},"source":["Exercise 2: visualize reconstructions.\n","- recall the basic plot functionality for a sample of MNIST digits in a batch of the test set \n","- evaluate qualitatively the model plotting the corresponding reconstructed images"]},{"cell_type":"code","source":["#we plot the first 30 digits in the testloader batch\n","inputs,labels = next(iter(testloader))\n","\n","fig=plt.figure(figsize=(15,10))\n","for i in range(30):\n","    plt.subplot(5,6,i+1)\n","    plt.imshow(np.squeeze(inputs[i]),cmap='bone')\n","    plt.xticks([])\n","    plt.yticks([])"],"metadata":{"id":"LGPAFbUvf66K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#compare with the reconstruction by performing inference\n","'''\n","Your code here\n","'''\n"],"metadata":{"id":"3b9BVZRPgKow"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cx3pWO-tBixa"},"source":["Inspect the ability of the autoencoder to interpolate between different digits moving along a line in the latent space. Explore how different interpolation give drastically different results by changing digit type."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yTr2fTFRBixb"},"outputs":[],"source":["autoencoder.eval()\n","\n","def interpolation(lambda1, model, img1, img2):\n","    \n","    with torch.no_grad():\n","\n","        # latent vector of first image\n","        img1 = img1.to(device)\n","        latent_1 = model.encoder(img1)\n","        # latent vector of second image\n","        img2 = img2.to(device)\n","        latent_2 = model.encoder(img2)\n","\n","        # interpolation of the two latent vectors\n","        inter_latent = lambda1 * latent_1 + (1- lambda1) * latent_2\n","        # reconstruct interpolated image\n","        inter_image = model.decoder(inter_latent)\n","        inter_image = inter_image.cpu()\n","    \n","    return inter_image\n","    \n","# sort part of test set by digit\n","digits = [[] for _ in range(10)]\n","for img_batch, label_batch in testloader:\n","    for i in range(img_batch.size(0)):\n","        digits[label_batch[i]].append(img_batch[i:i+1])\n","    if sum(len(d) for d in digits) >= 1000:\n","        break;\n","\n","# interpolation lambdas\n","lambda_range=np.linspace(0,1,10)"]},{"cell_type":"code","source":["fig, axs = plt.subplots(2,5, figsize=(15, 6))\n","fig.subplots_adjust(hspace = .5, wspace=.001)\n","axs = axs.ravel()\n","\n","digit1 = 1\n","digit2 = 7\n","for ind,l in enumerate(lambda_range):\n","    inter_image=interpolation(float(l), autoencoder, digits[digit1][0], digits[digit2][0])\n","    axs[ind].imshow(inter_image[0,0,:,:].cpu().detach().numpy(), cmap='bone')\n","    axs[ind].set_title('lambda_val='+str(round(l,1)))\n","\n","plt.show() "],"metadata":{"id":"b4y4vB5eySzS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FbjFF-e5Bixb"},"source":["Exercise 3. Can an autoencoder be considered an image generator?\n","\n","- Consider a batch of images in the test set. Compute the latent codes by means of the encoder.\n","- Compute the mean $\\mu$ and variance $\\Sigma$ of the latent codes obtained from the encoding.\n","- Sample 128 elements in the latent space sampling from $\\mathcal{N}(\\mu,\\Sigma)$. Compute the images corresponding to the codes by means of the decoder.\n","- Plot the result similarly as above. \n","\n","\n","You should observe that even if we are in the same region of the latent space as the encoded data, but even so, generating new digits is hit-or-miss: many latent vectors decode to something that does not look like a digit. \n","\n","This means that the manifold of latent vectors that decode to valid digits is sparse in latent space. With higher-dimensional latent spaces, the manifold gets sparser."]},{"cell_type":"code","source":["autoencoder.eval()\n","#Probably you will need to disable gradient calculation for doing this smoothly \n","'''\n","with torch.no_grad():\n","  Your code here\n","'''\n"],"metadata":{"id":"3MpRmRKSzeuq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Extra Exercise:\n","\n","- Monitor the hidden representations of the autoencoder following the analysis of notebook 08-representations.ipynb (i.e. visualisation of convolution outputs, TSNE projections, intrinsic dimension) "],"metadata":{"id":"tk0V5soO7uff"}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"autoencoders.ipynb","provenance":[{"file_id":"https://github.com/smartgeometry-ucl/dl4g/blob/master/autoencoder.ipynb","timestamp":1653495731811}],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}