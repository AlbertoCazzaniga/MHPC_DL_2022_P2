{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Basics of Autograd in Python (and first overview of _backpropagation_)\n",
    "\n",
    "What did we see last time?\n",
    "\n",
    "- PyTorch basics\n",
    "- Construction of a multilayer perceptron using PyTorch API\n",
    "\n",
    "**What caught our attention?**\n",
    "\n",
    "![](imgs/02/grad.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiation in PyTorch\n",
    "\n",
    "PyTorch is built with support for differentiation in mind.\n",
    "In the end, Deep Learning (for now) is all about differentiation and building cascades of differentiable function into complicated multilayer deep neural networks.\n",
    "\n",
    "Essentially, all PyTorch built-ins support differentiability (unless the function is not differentiable, of course).\n",
    "Today we will see how to compute derivatives in PyTorch.\n",
    "Also, we will learn how to create differentiable modules using PyTorch APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notation and recall\n",
    "\n",
    "1. **Function** $f:\\mathbb{R}\\rightarrow\\mathbb{R}$, given $x\\in\\mathbb{R}$, derivative is $\\frac{\\partial f}{\\partial x}$\n",
    "2. **Scalar function** $f:\\mathbb{R}^d\\rightarrow\\mathbb{R}$, we have a vector $\\mathbf{x}\\in\\mathbb{R}^d = (x_1,\\dots,x_d)$, we calculate the derivative of $f$ w.r.t. each of the dimensions of $\\mathbf{x}$ and obtain the gradient $\\nabla_f = (\\frac{\\partial f}{\\partial x_1},\\dots,\\frac{\\partial f}{\\partial x_d})$\n",
    "3. **Vector function** $f:\\mathbb{R}^d\\rightarrow\\mathbb{R}^k$, given $\\mathbf{x}$, we have $f(\\mathbf{x})=(f_1(\\mathbf{x}),\\dots,f_k(\\mathbf{x}))$, hence we can calculate $k$ gradients which we can gather in the Jacobian: $J_f=\\begin{pmatrix}\\frac{\\partial f_1}{\\partial x_1} & \\dots & \\frac{\\partial f_1}{\\partial x_d}\\\\\\vdots&\\ddots&\\vdots\\\\\\frac{\\partial f_k}{\\partial x_1} & \\dots & \\frac{\\partial f_k}{\\partial x_d}\\end{pmatrix} \\in \\mathbb{R}^{d\\times k}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grad functionality\n",
    "\n",
    "\"Under-the-hood\", each PT Tensor has an attribute `requires_grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3195, 0.1809, 0.9280],\n",
       "        [0.9200, 0.1693, 0.1443],\n",
       "        [0.1661, 0.4556, 0.7121]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(3,3)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually set this to `True` or create directly a Tensor supporting grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3195, 0.1809, 0.9280],\n",
       "        [0.9200, 0.1693, 0.1443],\n",
       "        [0.1661, 0.4556, 0.7121]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad = True\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9883, 0.7529, 0.9830],\n",
       "        [0.8041, 0.5193, 0.2135],\n",
       "        [0.3283, 0.3054, 0.3310]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3, 3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1\n",
    "\n",
    "Suppose we are in case 1.: $f:\\mathbb{R}\\rightarrow\\mathbb{R}$.\n",
    "\n",
    "For instance, $f(x) = x^2$.\n",
    "\n",
    "We could apply $f$ to a singleton tensor and calculate the derivative.\n",
    "\n",
    "We expect the derivative to be... ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([0.8687], requires_grad=True)\n",
      "y: tensor([0.7546], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, requires_grad=True)\n",
    "\n",
    "print(\"x:\", x)\n",
    "\n",
    "y = x**2\n",
    "\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the gradient, we call `backward()` on the Tensor. Which one, `x` or `y`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the gradient of x\n",
    "by accessing its grad attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7374])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that it's correct..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, when there's no gradient, it is automatically set to `None` to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3,3).grad is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2 (scalar function)\n",
    "\n",
    "We can use the same `.backward()` call to get the gradient of a scalar function.\n",
    "\n",
    "Now x will be a vector (or a matrix, it doesn't really matter for our case) and we will apply to it a function which returns a single scalar.\n",
    "\n",
    "One example may be $f(\\mathbf{x})=\\sum_{i=1}^d x_i$.\n",
    "\n",
    "**Q**: What is the gradient we expect to obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([5], requires_grad=True)\n",
    "\n",
    "y = x.sum()\n",
    "\n",
    "y.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3 (vector function)\n",
    "\n",
    "Unfortunately, the backward computation of the gradient is not directly capable of calculating the gradient for a vector of values, but only for a single scalar.\n",
    "\n",
    "If we wanted to compute the gradient on a vector function, what could we do?\n",
    "\n",
    "1. There exist a forward differentiation, which is not though present in PT, which lets us calculate derivative of vector functions with one-dimensional input.\n",
    "2. Using PT backward functionality in a loop through all the outputs\n",
    "   ```\n",
    "   # d is dimension of inputs\n",
    "   # k is dimension of outputs\n",
    "   outputs = model(inputs)\n",
    "   gradients = torch.empty((d, k)) # create empty d x k matrix\n",
    "   for i in range(k):\n",
    "       outputs[i].backward()\n",
    "       gradients[:, i] = inputs.grad\n",
    "       inputs.grad = None\n",
    "   ```\n",
    "\n",
    "**Q**: Why is really the backward differentiation (and not the forward) useful for our case?\n",
    "\n",
    "It is useful because we have a model whose ouput gets passed through a loss function $\\mathcal{L}$, which is always a scalar. So we can see the process as a composite function $l = \\mathcal{L}(f(X))$, where $f$ is our generic Machine Learning model.\n",
    "For deep learning, what we need to calculate is $\\partial \\mathcal{L}/\\partial \\Theta$ where $\\Theta$ are the parameters of our model (in an MLP, that is the collection of weights and biases).\n",
    "\n",
    "So we don't actually need to compute jacobians.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composition of functions\n",
    "\n",
    "We can use also `backward` to compute the gradient of a composition of functions. For our objective, it will be very useful to think in terms of computational graph.\n",
    "\n",
    "We can view $y=g(f(x))$ as\n",
    "\n",
    "![](imgs/02/compgra1.jpg)\n",
    "\n",
    "We might extend this and add a hidden node $z$\n",
    "between $f$ and $g$\n",
    "\n",
    "![](imgs/02/compgra2.jpg)\n",
    "\n",
    "Supposing $f(x)=log(x)$\n",
    "and $g(x)=x^2$, we can reproduce this example in PyTorch.\n",
    "\n",
    "**Q**\n",
    "\n",
    "- What we expect to get from $\\partial g/\\partial z$? $2z$ which is equal to $2\\log(x)$\n",
    "\n",
    "- And from $\\partial f/\\partial x$? $1/x$\n",
    "\n",
    "- And from $\\partial g/\\partial x$? $2 \\log (x) / x$\n",
    "\n",
    "- More specifically, what technique do we use to calculate this final gradient? The chain rule ($\\partial g/\\partial x = \\partial g/\\partial z \\cdot \\partial f/\\partial x$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([0.9460], requires_grad=True) \n",
      "\n",
      "y: tensor([0.0031], grad_fn=<PowBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.rand(1, requires_grad=True)\n",
    "\n",
    "print(\"x:\", x, \"\\n\")\n",
    "\n",
    "z = x.log()\n",
    "\n",
    "y = z**2\n",
    "\n",
    "print(\"y:\", y, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by printing `y`, we can see that the tensor has a specific gradient function attached.\n",
    "\n",
    "Let us now compute the gradient..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of x: tensor([-0.1174]) \n",
      "Q:(gradient of x w.r.t. what?)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y.backward()\n",
    "\n",
    "print(\"gradient of x:\", x.grad, \"\\nQ:(gradient of x w.r.t. what?)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us access $\\partial g/\\partial z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zullich/miniconda3/envs/lot/lib/python3.8/site-packages/torch/_tensor.py:1013: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272128894/work/build/aten/src/ATen/core/TensorBody.h:417.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "z.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: to store gradients of intermediate computations, we can call `.retain_grad()` on the intermediate node.\n",
    "Example:\n",
    "\n",
    "```\n",
    "z.retain_grad()\n",
    "\n",
    "y.backward()\n",
    "\n",
    "z.grad -> now it won't be None\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more complicated example\n",
    "\n",
    "![](imgs/02/compgra3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_1:   tensor([3.], requires_grad=True)\n",
      "x_2:   tensor([2.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x_1 = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "x_2 = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "print(\"x_1:  \", x_1)\n",
    "print(\"x_2:  \", x_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct `c`, calculate the gradient and access it for both `x_1` and `x_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0978])\n",
      "tensor([-0.4950])\n"
     ]
    }
   ],
   "source": [
    "c = x_1.cos() * x_2.log()\n",
    "c.backward()\n",
    "print(x_1.grad)\n",
    "print(x_2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient accumulation\n",
    "\n",
    "Let us see another feature of torch differentiation functionalities.\n",
    "\n",
    "We can call `backward()` multiple times; let us see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1956]) tensor([-0.9900])\n"
     ]
    }
   ],
   "source": [
    "c = x_1.cos() * x_2.log()\n",
    "c.backward()\n",
    "print(x_1.grad, x_2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: what is happening? Why the gradient is not the same?\n",
    "\n",
    "They doubled. PyTorch continues to accumulate (i.e., sum) the gradients. If we want to reset the gradient, we must set it to None\n",
    "```\n",
    "x_1.grad = None\n",
    "x_2.grad = None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a custom, non-parametric PyTorch module\n",
    "\n",
    "Basically, we want to create a module which is not controlled by any parameter, be it trainable or non-trainable.\n",
    "\n",
    "As an example, we might have the **Leaky ReLU**, an activation function which can be used in place of the more-known ReLU.\n",
    "\n",
    "$\\text{LeakyReLU} = \\max\\{0.01\\cdot x, x\\}$\n",
    "\n",
    "![](https://i1.wp.com/clay-atlas.com/wp-content/uploads/2019/10/image-37.png?resize=640%2C480&ssl=1)\n",
    "\n",
    "We can construct it like a basic PyTorch module, analogously to the MultiLayer Perceptron which we built (but not trained) at the end of Lab 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return torch.max(data, data*0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and that's it. We may plug it into a neural network module and it'll work just fine, both for the forward and backward pass.\n",
    "\n",
    "If we want, we can also use it as-is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1000, -0.0900, -0.0800, -0.0700, -0.0600, -0.0500, -0.0400, -0.0300,\n",
       "        -0.0200, -0.0100,  0.0000,  1.0000,  2.0000,  3.0000,  4.0000,  5.0000,\n",
       "         6.0000,  7.0000,  8.0000,  9.0000])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_relu = LeakyReLU()\n",
    "\n",
    "leaky_relu(torch.arange(-10,10)) # is identical to leaky_rely.forward(torch.arange(-10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us test its autodiff functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor(0.9900, grad_fn=<SumBackward0>)\n",
      "dy/dx: tensor([1.0000, 0.0100])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, -1.0], requires_grad=True)\n",
    "\n",
    "y = leaky_relu(x).sum() # sum to get one single value out of it.\n",
    "\n",
    "print(\"y:\", y)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(\"dy/dx:\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the gradient gets calculated automatically without our intervention in defining a gradient function.\n",
    "\n",
    "But what if that was not already implemented in PyTorch? What if we needed to use some function that cannot be constructed by using PyTorch built-ins?\n",
    "\n",
    "In this case, we must define a function class which inherits from `torch.autograd.Function`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autograd Function inherits from `torch.autograd.Function` and has two compulsory methods: `forward` and `backward`, whose meaning should be obvious to all.\n",
    "\n",
    "Both functions have a compulsory first argument which is the **context**, `ctx` for brevity.\n",
    "From the context we can infer informations about the entities involved in the calculation of the gradient.\n",
    "\n",
    "The context is **built upon calling the `forward` method**, so that, during the `backward` call, we can obtain the info such what tensors have been used in `forward` and whether a tensor requires or not the grad.\n",
    "\n",
    "In our case, the derivative is the following:\n",
    "$\\frac{\\partial\\text{LeakyReLU}}{\\partial x} = \\begin{cases} 1\\text{ if }x>0 \\\\ 0.01\\text{ if }x\\leq 0\\end{cases}$, so we only need to save $x$, i.e., the data coming into the module.\n",
    "\n",
    "Moreover, the backward method needs an additional argument, `output_grad`, which conveys information about the gradient which is _entering_ the Function (be mindful, we're running _backward_, so a gradient _enters the function_ upstream w.r.t. the forward pass).\n",
    "\n",
    "This is necessary in order to build a cascade of sequential module, each applied after the other. This calls for the application of the **chain rule** for the computation of the gradient of **compositions of functions**:\n",
    "\n",
    "$$\n",
    "z = g(f(x)): \\\\\n",
    "y = f(x) \\wedge z = g(y) \\\\\n",
    "$$\n",
    "\n",
    "![](imgs/02/compgra_forward.jpg)\n",
    "\n",
    "Then, switching to the derivative:\n",
    "\n",
    "$\\Rightarrow \\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\cdot\\frac{\\partial y}{\\partial x} $\n",
    "\n",
    "![](imgs/02/compgra_backward2.gif)\n",
    "\n",
    "So, it becomes immediately overt the necessity of having an **incoming** gradient which you use to multiply with the gradient produced by the current module, the result of which gets passed on to the previous node in the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU_Fun(torch.autograd.Function):\n",
    "    @staticmethod # mind the decorator\n",
    "    def forward(ctx, input_):\n",
    "        ctx.save_for_backward(input_) # the parameters that will be involved in the gradient\n",
    "        return torch.max(input_, input_ * 0.01)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_, = ctx.saved_tensors # these are the variables which we need to backpropagate the gradient to (only the input)\n",
    "        # the gradient is 1 for positive x's, 0.01 for negative x's\n",
    "        grad_input = torch.ones_like(input_)\n",
    "        grad_input[input_<0] = 0.01\n",
    "        # now, we need to rescale for the grad_output\n",
    "        grad_input *= grad_output\n",
    "        '''\n",
    "        a valid alternative (maybe better performing?):\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input_<0] *= 0.01\n",
    "        '''\n",
    "        return grad_input\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = LeakyReLU_Fun.apply\n",
    "x = torch.linspace(-5,5,11, requires_grad=True)\n",
    "y = fun(x)\n",
    "z = y.sum()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us then rivisit our `LeakyReLU` module from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU_Better(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return LeakyReLU_Fun.apply(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0500, -0.0400, -0.0300, -0.0200, -0.0100,  0.0000,  1.0000,  2.0000,\n",
       "         3.0000,  4.0000,  5.0000], grad_fn=<LeakyReLU_FunBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LeakyReLU_Better()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a custom parametric module\n",
    "\n",
    "We wish to extend our Leaky ReLU module to the Parametric ReLU: $\\text{ParamReLU} = \\max\\{\\alpha\\cdot x, x\\}, \\alpha \\in [0,1)$.\n",
    "\n",
    "![](https://pytorch.org/docs/stable/_images/PReLU.png)\n",
    "\n",
    "Parametric ReLU with $\\alpha=0.25$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamReLU_Fun(torch.autograd.Function):\n",
    "    @staticmethod # mind the decorator\n",
    "    def forward(ctx, input_, alpha:float):\n",
    "        assert alpha >= 0 and alpha < 1, f\"alpha should be >= 0 and < 1. Found {alpha}.\"\n",
    "        ctx.save_for_backward(input_) # the parameters that will be involved in the gradient\n",
    "        ctx.alpha = alpha # note that we don't use self.alpha\n",
    "        return torch.max(input_, input_ * alpha)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_, = ctx.saved_tensors # these are the variables which we need to backpropagate the gradient to (only the input)\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input_<0] *= ctx.alpha\n",
    "        return grad_input, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamReLU(torch.nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return ParamReLU_Fun.apply(X, self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "prelu = ParamReLU(0.25)\n",
    "x = torch.linspace(-5,5,11, requires_grad=True)\n",
    "y = prelu(x)\n",
    "z = y.sum()\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have covered:\n",
    "\n",
    "1. The construction of a non-parametric differentiable module\n",
    "2. The construction of a parametric, non-trainable, differentiable module\n",
    "\n",
    "What's missing?\n",
    "\n",
    "A construction of a parametric trainable and differentiable model, which we will (hopefully) see once we know how to train models with SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra\n",
    "\n",
    "**Backpropagation**\n",
    "\n",
    "Let us suppose we have the following calculation\n",
    "\n",
    "$\\mathbf{x} = [1,~2,~-1,~3,~5]$\n",
    "\n",
    "$ y = f(\\mathbf{x}) = \\log\\{[\\exp (x_1 * x_2 )]^2 + \\sin (x_3 + x_4 + x_5) \\cdot x_5\\}$\n",
    "\n",
    "_**See video in the Teams files for manual backpropagation!**_\n",
    "\n",
    "Find\n",
    "\n",
    "$\\nabla f(\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it in Python as homework!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation with PyTorch modules**\n",
    "\n",
    "$\\mathbf{x} = [1,~2,~-1,~3,~5]^\\top,~~\\mathbf{w} = [3,~0,~1,~-3,~0.5]^\\top$\n",
    "\n",
    "$y = \\sigma(\\mathbf{w}^\\top \\mathbf{x})$, where $\\sigma$ is the sigmoidal function $\\frac{1}{1+\\exp(-x)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrivialBackpropagationExample(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.module1 = # what goes here?\n",
    "        self.module1.weight.data = torch.Tensor([3, 0, 1, -3, .5])\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return torch.sigmoid(self.module1(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that the loss function is the binary cross-entropy\n",
    "\n",
    "$$BCE(\\hat{y}, y) = -\\frac{1}{N} \\sum_{i=1}^n [ y_i\\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i) ]$$\n",
    "\n",
    "and that the ground truth is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(y_hat, y):\n",
    "    return (-1/len(y)) * (y_hat * y.log() + (1 - y_hat) * (1 - y.log())).sum()\n",
    "\n",
    "ground_truth = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrivialBackpropagationExample()\n",
    "\n",
    "x = torch.tensor([1,2,-1,3,5], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "y_hat = model(x)\n",
    "\n",
    "loss = bce_loss(y_hat, ground_truth)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "1. Complete the Python implementation of the backpropagation exercise in the **Backpropagation** section here above (cell `# try it in Python as homework!`)\n",
    "    - Create the calculations for obtaining $y$ in PyTorch **using only PyTorch methods and routines**\n",
    "    - Calculate the gradient\n",
    "    - Check the values of the gradients and see if it is correct w.r.t. the manual calculations\n",
    "2. Given the multilayer perceptron defined during the exercises from lab 1:\n",
    "    - Create 10 random datapoints (with any function you wish, it can be `rand`, `randn`...) and feed them into the network\n",
    "    - Given the output, calculate the Cross-Entropy loss with respect to the ground truth $[1,2,3,4,1,2,3,4,1,2]$ (classes from 1 to 4). Cross-Entropy loss:\n",
    "        \n",
    "        $$ CE(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\frac{1}{10}\\sum_{i=1}^{10} y_i^\\top \\log(\\hat{y}_i)$$\n",
    "        \n",
    "        where $y_i$ is the one-hot encoding of the $i$-th datapoint. For instance, $y_1 = [1,0,0,0]^\\top$.\n",
    "        **_Note: there is an extremely handy PyTorch function for getting a one-hot encoding out of a vector, so don't try anything fancy._**\n",
    "    - Backpropagate the error along the network and inspect the gradient of the parameters connecting the input layer and the first hidden layer.\n",
    "3. Execute the python script `utils/randomized_backpropagation_formula.py`. This creates a formula $f(\\mathbf{x})$ with randomized operators and values. Create the computational graph from this formula, do (by hand) the forward pass, then calculate (by hand) $\\nabla f(\\mathbf{x})$ using the backward gradient computation. Do the same calculation on PyTorch to check the correctness of your calculations. _Note: The formula created by this script is linked to your name and surname, which you have to input before_. The solution to this exercise _should_ be submitted as a scan/good quality picture of a piece of paper (or you can do it on a touch screen and submit the image...), but other formats are acceptable as well.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b77c29885b974dcfddc958349edd2398568367665d9f0a08534c65bb7a1b37b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "8aec2e4cca6a43ecda9b11f31ea0f9f4b012d28e6de8cbdf64a5e136ca9a5fb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
