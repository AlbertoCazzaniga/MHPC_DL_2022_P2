{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"08-representations.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### PROPERTIES AND EXPLORATION OF HIDDEN *REPRESENTATIONS*\n"],"metadata":{"id":"PQrJZ5rlMGs5"}},{"cell_type":"markdown","source":["MAIN OBJECTIVES: \n","- extract hidden representation customising a standard model \n","- discuss qualitatively few basic techniques for analysis of *representations* "],"metadata":{"id":"j35RHoDUMJ5g"}},{"cell_type":"markdown","source":["The next few blocks restart from Day2 setting up the basics for training the models we are going to inspect later:\n","\n","- we make sure folders in the course repo are available;\n","- we load the MNIST dataset and check the data looks as expected;\n","- we define basic training and test functionalities we will use for learning the weights of our model\n","- we define standard training hyperparameters "],"metadata":{"id":"uIZDUtz_MRfF"}},{"cell_type":"code","source":["# comment if access to the course repo is already available\n","!git clone https://github.com/AlbertoCazzaniga/MHPC_2022_test"],"metadata":{"id":"7jDavRjIMAbi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# comment if access to the course repo is already available\n","%cd MHPC_2022_test"],"metadata":{"id":"6JzGzmr8MH3v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# basic imports, load MNIST\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torchsummary import summary\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from scripts import mnist #NOTE: might need to customize this\n","\n","minibatch_size_train = 64\n","minibatch_size_test = 1000\n","\n","trainloader, testloader = mnist.get_data(batch_size_train=minibatch_size_test, batch_size_test=minibatch_size_test)"],"metadata":{"id":"p6swM7SnOGIm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# a quick look at MNIST images\n","inputs,labels = next(iter(testloader))\n","\n","fig=plt.figure(figsize=(15,10))\n","for i in range(30):\n","    plt.subplot(5,6,i+1)\n","    plt.imshow(np.squeeze(inputs[i]),cmap='bone')\n","    plt.xticks([])\n","    plt.yticks([])"],"metadata":{"id":"h1hVN8RBMdF6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#make sure we handle correctly hardware resources depending on availability, and fix seed\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"],"metadata":{"id":"lR-VLB1vMf96"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#fix basic training settings, and fix seed. NOTE: modify as you please!\n","seed = 33\n","torch.manual_seed(seed)\n","input_size=(1,28,28,)\n","epochs=15\n","lr=0.01\n","momentum=0.0   \n","log_interval=100"],"metadata":{"id":"EnqMCywlMjEf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#set up basic train and test functionalities\n","def train(model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.nll_loss(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","\n","def test(model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"metadata":{"id":"FCTrQve1Ml5G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#define convolutional network with\n","class ConvNet(nn.Module):\n","    def __init__(self):\n","        super(ConvNet, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n","        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n","        self.fc1 = nn.Linear(4*4*50, 500)\n","        self.fc2 = nn.Linear(500, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.max_pool2d(x, 2, 2)\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x, 2, 2)\n","        x = x.view(-1, 4*4*50)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return F.log_softmax(x, dim=1)"],"metadata":{"id":"drd2yGCGMqoI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = ConvNet().to(device)\n","summary(model,input_size)"],"metadata":{"id":"h7r9ZHxGM0_m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","output = model(inputs.to(device))\n","pred = output.argmax(dim=1, keepdim=True) \n","correct = pred.eq(labels.to(device).view_as(pred)).sum().item()\n","print('acc = {}'.format(correct/inputs.shape[0]) )"],"metadata":{"id":"I3dnFE2NM5GP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n","save=True"],"metadata":{"id":"Pn_bf8diNA5H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(1, epochs + 1):\n","    train(model, device, trainloader, optimizer, epoch)\n","    test(model, device, testloader)\n","    \n","if save:\n","    torch.save(model.state_dict(),\"mnist_cn2.pt\")"],"metadata":{"id":"TkS8AO4pNE_S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Exercise 1: Basics.\n","- Define a model ConvNetReps with an additional attribute to extract the hidden representations of the model ConvNet.\n","- Define an instance of ConvNetReps and load weights obtained from training of ConvNet\n","- Extract representations for a batch of the testloader, and inspect dimensions"],"metadata":{"id":"YkFKfR9WQQmn"}},{"cell_type":"code","source":["#define convolutional network with extract method\n","class ConvNetReps(nn.Module):\n","    def __init__(self):\n","        super(ConvNetReps, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n","        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n","        self.fc1 = nn.Linear(4*4*50, 500)\n","        self.fc2 = nn.Linear(500, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.max_pool2d(x, 2, 2)\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x, 2, 2)\n","        x = x.view(-1, 4*4*50)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return F.log_softmax(x, dim=1)\n","    \n","    '''\n","    Your code here\n","    def extract(self,...):\n","        Input: like MNIST image \n","        Output: list of 9 tensors each containing a representation \n","                of the forward loop -input and output included!-\n","    '''"],"metadata":{"id":"d3s2tAdjQP7t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#define instance of ConvNetReps(), load weights, set in evaluation mode\n","'''\n","Your code here\n","'''"],"metadata":{"id":"bt4ndhtATNbd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#load a batch of inputs from the testloader\n","#extract representations using method defined above\n","#inspect dimensions\n","'''\n","Your code here\n","'''"],"metadata":{"id":"RoTvZA5YUgdb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Exercise 2: Visualisation.\n","\n","Note that after each convolution each channel can be interpreted as an image. \n","- Choose an image and plot the first 20 channels for the first 4 layers. What does it show about the level of abstraction?\n","\n","\"T-distributed Stochastic Neighbor Embedding\" [TSNE-sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) allows to project high dimensional data onto 2-d allowing simple visualisation that preserves some of the properties of the original data.\n","- Use TSNE to inspect the geometry of some layers (e.g. layers after second maxpooling, two last hidden layers).\n","\n","- Take home message &/or reading exercise: TSNE has strenght and limitations [Distill-TSNE](https://distill.pub/2016/misread-tsne/)"],"metadata":{"id":"eRSlFGUGYfQ9"}},{"cell_type":"code","source":["#visualize channel images\n","'''\n","Your code here\n","'''"],"metadata":{"id":"f0LA_42xVi2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#tsne geometry inspection\n","from sklearn.manifold import TSNE\n","'''\n","Your code here\n","'''"],"metadata":{"id":"xefgNEFSZgSq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Exercise 3: Intrinsic dimension\n","\n","Monitoring the dimension of the representations of a dataset in the hidden layers of a DNN can reveal information about the learning procedure (see for instance [Ansuini et al., 2019](https://proceedings.neurips.cc/paper/2019/file/cfcce0621b49c983991ead4c3d4d3b6b-Paper.pdf) ).\n","\n","We consider the TWO-NN algorithm for estimating the intrinsic dimension [Facco et al., 2018](https://www.nature.com/articles/s41598-017-11873-y.pdf) of the representations in various hidden layers. The algorithm only uses the distance among the datapoints.\n","\n","- The ID estimation is as more reliable as more points we consider. Modify the definition of test loader to obtain batches of 5000. Extract representations. \n","- Start by the input representation of this batch: compute the distance matrix using SciPy [pdist](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html) and [squareform](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.squareform.html) functions.\n","- Use the function block_analysis as indicated to compute the decimation curve of ID and the corresponding errors.\n","- Do the same for other hidden representations (I suggest 3,6,7 -why?-). Plot the curves together and discuss the behaviour of the ID in the hidden representations of the model."],"metadata":{"id":"4VGl_AdyZgpn"}},{"cell_type":"code","source":["#import relevant function for ID computation\n","from intrinsic_dimension import estimate,block_analysis\n","#import relevant function for distance matrix computation\n","from scipy.spatial.distance import pdist,squareform"],"metadata":{"id":"xPnURSKLsoSn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#extract representation for a batch of the testloader\n","'''\n","Your code here\n","'''"],"metadata":{"id":"t9MoWToTuHGv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#compute distance matrix for the input dataset \n","#NOTE: need to understand how to compute distance between image tensors (1,28,28),\n","#using function block_analysis compute ID dimensions (output[0]), \n","#error estimates (output[1]) and number of points considered (output[2])\n","#in the decimation process\n","'''\n","Your code here\n","'''"],"metadata":{"id":"sLXDS0KS719N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#compute ID also for layers 3,6,7 and discuss qualitatively the behaviour\n","'''\n","Your code here\n","'''"],"metadata":{"id":"kSEVupjTuf-z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Extra Exercise:\n","\n","- Repeat the analysis for the hidden representations of MNIST obtained through your favourite MLP classifier. Compare qualitative the results to the one obtained with CNN."],"metadata":{"id":"o5R_lp24A2Cs"}},{"cell_type":"code","source":[""],"metadata":{"id":"yqFTmNwv74ml"},"execution_count":null,"outputs":[]}]}