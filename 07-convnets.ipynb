{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7 - Convolutional Neural Networks (CNNs)\n",
    "\n",
    "## The building blocks of CNNs\n",
    "\n",
    "### Convolutional layers\n",
    "\n",
    "The basic building block for a CNN is the convolutional layer, accessible as `torch.nn.Conv<s>d`, where `<s>` represents the number of **spatial dimensions** of our data:\n",
    "* `Conv1d` for 1 dimensional sequences. Example: audio. Audio is organized as a sequence of a given length (the single spatial dimension), where each single value in this sequence represent the intensity/amplitude of the signal for a given time point. Audio data can be organized in multiple **channels** (e.g., stereo data has 2 channels). The convolution opration is represented by a one-dimensional kernel;\n",
    "* `Conv2d` for 2 dimensional data, like images.\n",
    "* `Conv3d` for 3 dimensional data. An example might be a 3D reconstruction of an image. A convolution in that domain might equate to sliding a cubic kernel along all three dimensions.\n",
    "\n",
    "Parameters for constructors:\n",
    "```\n",
    "Conv2d(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int]] = 0, dilation: Union[int, Tuple[int, int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros')\n",
    "```\n",
    "* in_channels: the number of channels of the incoming data\n",
    "* out_channels: the number of channels for the output data, i.e., the number of convolutions that are operated\n",
    "* kernel_size: the kernel size of each convolution. An int $k$ is interpreted as a tuple $(k, k)$ (i.e., a square kernel); for a rectangular kernel, pass a tuple.\n",
    "* stride, padding, dilation: trivial\n",
    "\n",
    "#### Note that the convolution does NOT require a specific spatial dimension as input/output, as convolution is oblivious to these factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters of convolution\n",
      " Weights\n",
      " torch.Size([32, 3, 3, 3]) \n",
      "Bias\n",
      " torch.Size([32])\n",
      "\n",
      "Conv2d is applied independently of the input spatial dimension\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [32, 3, 3, 3], but got 3-dimensional input of size [3, 32, 32] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-59331ee7c149>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nConv2d is applied independently of the input spatial dimension\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shape of y \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 396\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [32, 3, 3, 3], but got 3-dimensional input of size [3, 32, 32] instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "print(\"Parameters of convolution\\n\", \"Weights\\n\", conv_layer.weight.shape, \"\\nBias\\n\", conv_layer.bias.shape)\n",
    "\n",
    "print(\"\\nConv2d is applied independently of the input spatial dimension\")\n",
    "y = conv_layer(torch.rand(1,3,32,32))\n",
    "print(\"Shape of y \", y.shape)\n",
    "\n",
    "z = conv_layer(torch.rand(1,3,12,12))\n",
    "print(\"Shape of z \", z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling layers\n",
    "\n",
    "Pooling layers are essentially convolutions without trainable kernels. For each overlap between the image and the kernel, they output the maximum (→ _maxpooling_) or the average (→ _avgpooling_) of the image in that specific region.\n",
    "\n",
    "![](https://production-media.paperswithcode.com/methods/MaxpoolSample2.png)\n",
    "\n",
    "```MaxPool2d(kernel_size: Union[int, Tuple[int, ...]], stride: Union[int, Tuple[int, ...], NoneType] = None, padding: Union[int, Tuple[int, ...]] = 0, dilation: Union[int, Tuple[int, ...]] = 1, return_indices: bool = False, ceil_mode: bool = False)```\n",
    "\n",
    "Notice that now we have no input or output channels as parameter, because MaxPool/AvgPool act independently on each channel, so `in_channels=out_channels`\n",
    "\n",
    "#### Adaptive Pooling\n",
    "\n",
    "Adaptive (Max/Average) Pooling is still a pooling layer, but we have the option to specify the desired spatial dimension of the output instead of the parameters like kernel size, padding...\n",
    "\n",
    "PyTorch works out by itself the params which are required in order for the pooling to produce an output of the desired size.\n",
    "\n",
    "Maybe the most common application of this layer is when operating the channel-wise average pooling at the end of the cascade of convolutional layers. In this case, we specify a fixed size of $(1,1)$, s.t. PyTorch will essentially operate an average of each whole channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "layer(torch.rand(1,3,32,32)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: what do we need to push this data through a Linear layer? We need to flatten the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Flatten()(layer(torch.rand(1,3,32,32))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the building blocks of a CNN, let's put it in practice and build something\n",
    "\n",
    "## Let's build a ResNet18!\n",
    "\n",
    "A ResNet is a CNN architecture which is composed by a cascade of \"residual blocks\", which are themselves cascades of convolutions, plus skip connections.\n",
    "\n",
    "The ResNet18 architecture is composed of 18 convolutional layers arranged in \"basic\" residual blocks with this shape:\n",
    "\n",
    "![](https://miro.medium.com/max/1140/1*D0F3UitQ2l5Q0Ak-tjEdJg.png)\n",
    "\n",
    "The incoming information gets duplicated: one flow passes through multiple convolutional layers (`weight layer` in the image above), while the other \"skips\" these layers and is summed up to the output of the last conv layer, _before_ the activation function of this layer is applied.\n",
    "\n",
    "Skip connections avoid the dilemma of the \"vanishing gradient\", where large neural network architectures are basically untrainable due to the first layers having extremely small gradients which limits weight update.\n",
    "\n",
    "This, instead, is the general scheme of a ResNet18:\n",
    "\n",
    "![](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fimg-blog.csdnimg.cn%2F20191128165045833.png%3Fx-oss-process%3Dimage%2Fwatermark%2Ctype_ZmFuZ3poZW5naGVpdGk%2Cshadow_10%2Ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTUxOTQ2Mw%3D%3D%2Csize_16%2Ccolor_FFFFFF%2Ct_70&f=1&nofb=1)\n",
    "\n",
    "Additionally, notice that all convolutional layers within residual blocks have **no bias**.\n",
    "\n",
    "Let us build, in order,\n",
    "\n",
    "1. The preparatory block\n",
    "2. The classifier block (pooling + linear)\n",
    "3. The basic residual block without downsampling\n",
    "   * then add downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 56, 56])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PreparatoryBlock(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        # conv with 7x7 kernel, stride 2, padding 3\n",
    "        # maxpool with 3x3 kernel, stride 2, padding 1\n",
    "        self.mod = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return self.mod(data)\n",
    "\n",
    "block = PreparatoryBlock()\n",
    "block(torch.rand(1,3,224,224)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ClassifierBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.mod = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_channels, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return self.mod(data)\n",
    "\n",
    "block = ClassifierBlock(512)\n",
    "block(torch.rand(1, 512, 7, 7)).shape\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 56, 56])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BasicResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsampling=False):\n",
    "        super().__init__()\n",
    "        stride_first_layer = (2 if downsampling else 1)\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1, stride=stride_first_layer ,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.downsampling = downsampling\n",
    "        if downsampling:\n",
    "            self.conv_skip = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=2, padding=1, bias=False)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        out1 = data\n",
    "        if self.downsampling:\n",
    "            out1 = self.conv_skip(out1)\n",
    "        out2 = self.conv1(data)\n",
    "        out2 = self.bn1(out2)\n",
    "        out2 = F.relu(out2)\n",
    "        out2 = self.conv2(out2)\n",
    "        out2 = self.bn2(out2)\n",
    "        out2 += out1\n",
    "        return F.relu(out2)\n",
    "\n",
    "block = BasicResidualBlock(64, 64)\n",
    "block(torch.rand(1, 64, 56, 56)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build a U-Net for image segmentation!\n",
    "\n",
    "![](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net is a network architecture for image segmentation (but it has worked well also in other domains, as image inpainting and image generation) which is composed of two main substructures:\n",
    "1. a shrinking phase, where a cascade of convolutions and maxpooling progressively reduces the size of the images\n",
    "   * each convolution has a padding of 0, so effectively also the regular convolution reduces the size of the image\n",
    "2. a growing phase, where convolutions are intertwined with transposed convolutions to increase the size of the images\n",
    "   * to mimic the behavior of the maxpooling in the shrinking phase, the transposed convolution has the same parameters: stride and kenrel size of 2, padding of 0.\n",
    "\n",
    "The two phases are organized in levels, s.t. the output of the final convolution of a shrinking level (before maxpool) gets cropped, copied, and concatenated to the input of the corresponding level of the growing phase (after transposed convolution).\n",
    "\n",
    "After the growing phase, we have the segmentation is performed by classifying each pixel of the resulting output image: in this setting, we have an image of size $h^\\prime\\times w^\\prime \\times C$ where $C$ is the number of classes, and we do a pixel-level softmax, so we're able to classify each pixel.\n",
    "\n",
    "To build a UNet, we will first build two structure:\n",
    "1. The Shrinking block\n",
    "2. The Growing block\n",
    "\n",
    "Then put all the pieces together in a final class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output:  torch.Size([1, 64, 284, 284])\n",
      "Shape of output for upsample:  torch.Size([1, 64, 568, 568])\n"
     ]
    }
   ],
   "source": [
    "class ConvBlockDownsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.mod = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "        self.downsample = nn.MaxPool2d(kernel_size=2)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        out = self.mod(data)\n",
    "        out2 = self.downsample(out)\n",
    "        return out, out2\n",
    "\n",
    "block = ConvBlockDownsample(3, 64)\n",
    "output_for_upsample, output = block(torch.rand(1, 3, 572, 572))\n",
    "print(\"Shape of output: \", output.shape)\n",
    "print(\"Shape of output for upsample: \", output_for_upsample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 392, 392])\n",
      "392 392\n",
      "568 568\n",
      "torch.Size([1, 64, 392, 392])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 388, 388])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConvBlockUpsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, data, data_from_downsample:torch.Tensor):\n",
    "        out = self.upsample(data)\n",
    "        # data has dimension ch x h x w\n",
    "        # data_from_downsample has dimension ch x H x W\n",
    "        # with H > h, W > w\n",
    "        h, w = out.shape[2], out.shape[3]\n",
    "        H, W = data_from_downsample.shape[2], data_from_downsample.shape[3]\n",
    "        # do a center crop of data_from_downsample \n",
    "        # (starting from H//2, W//2, the center pixel of the larger image)\n",
    "        cropped_data_from_downsample = data_from_downsample[:, :, H//2-h//2 : H//2+(h//2 + h%2), W//2-w//2 : W//2+(w//2 + w%2)]\n",
    "        out = torch.cat([out, cropped_data_from_downsample], dim=1)\n",
    "        return self.conv(out)\n",
    "        \n",
    "block = ConvBlockUpsample(128, 64)\n",
    "data_before_upsample = torch.rand((1, 128, 196, 196))\n",
    "# we do the forward pass by output_for_upsample from before\n",
    "block(data_before_upsample, output_for_upsample).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and additional material\n",
    "\n",
    "* [tutorial for building a LeNet CNN on KMNIST (+ training)](https://pyimagesearch.com/2021/07/19/pytorch-training-your-first-convolutional-neural-network-cnn/)\n",
    "* ResNets: [Deep Residual Networks for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
    "* UNet: [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf508bf4ee7112a2ed8a720eac4d4b7b0db0d37a6aeaf9014b273272b42dbc2f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
